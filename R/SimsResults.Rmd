---
title: "Simulation results - Non-parametric efficient estimation of marginal structural models with multi-valued time-varying treatments"
output: 
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.width=12, fig.height=8)
library(here)
library(dplyr)
library(reshape2)
library(dplyr)
library(ggplot2)
library(tibble)
library(np)
library(dplyr)
library(parallel)
library(SuperLearner)
library(cubature)
library(ggplot2)
library(reshape2)
library(tidyr)
library(ggpubr)
library(data.table)
library(simcausal)
library(mc2d)
library(here)
library(gt)
library(nnet)
library(ggdag)
library(dagitty)
source(here("R/summarise_sims.R"))
```

# Introduction

This document summarises the simulation results presented in the manuscript Non-parametric efficient estimation of marginal structural models with multi-valued time-varying treatments, including all tables in figures.

The approximate true values are as follows:

```{r}
load(here("data/true_discrete_cat5_simple_mult.Rdata"))
```

- U1 value `r truevalue$U1`

- U2 value: IPW: `r truevalue$ipw`; Nelder-Mead optimization: `r truevalue$par_NM`; CG optimization: `r truevalue$par_CG`


I also generated the numerator of the stabilized weights using a multinomial model (using only prior treatment history as covariates), we find similar results:

- U1 value `r truevalue$U1_la`

- U2 value: IPW: `r truevalue[[8]]`; Nelder-Mead optimization: `r truevalue$par_NM_la`; CG optimization: `r truevalue$par_CG_la`

But the weights from the stabilized weights are much more stable than the non-stabilized ones:

- non-stabilized:

```{r}
truevalue$trueweights
```

- stabilized:

```{r}
truevalue$trueweights_la
```


Of note I also recorded the expected bias that would be observed (by estimating treatment probabilities the same way we do in the simulations), it leads to the following values: `r truevalue$ipw_bias_la`. 




# Simulation results


This section is subcategorized in 2 parts because we initial choices of learners lead to a very slow run time and it seemed like I would not get results in a reasonable amount of time. I then switched to a more basic set of learners to get results faster. The first section reflects this limited set of learners, the second section reflects the extended set of learners.


## Quick runs

Done with 200 iterations of each sample size.

```{r, echo = T, eval = F}
stackr = list("mean", "lightgbm", "multinom","xgboost", "nnet",
              "knn", "rpart", "naivebayes","glmnet",
              list("randomforest", ntree = 250, id = "randomforest"),
              list("ranger", num.trees = 250, id = "ranger")
)

stackm = c('SL.mean','SL.glmnet','SL.earth','SL.glm.interaction')
```


```{r}
# load the results #
load(here("results/Mult_n250.Rdata"))
res_250 <- res
load(here("results/Mult_n500.Rdata"))
res_500 <- res
load(here("results/Mult_n1000.Rdata"))
res_1000 <- res
load(here("results/Mult_n2000.Rdata"))
res_2000 <- res
load(here("results/Mult_n3000.Rdata"))
res_3000 <- res
load(here("results/Mult_n4000.Rdata"))
res_4000 <- res
load(here("results/Mult_n5000.Rdata"))
res_5000 <- res

### estimated ###
est_250 <- lapply(1:length(res_250), function(x){
  if(is.character(res_250[[x]][[1]])) return(NULL) else res_250[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 250)

est_500 <- lapply(1:length(res_500), function(x){
  if(is.character(res_500[[x]][[1]])) return(NULL) else res_500[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 500)

est_1000 <- lapply(1:length(res_1000), function(x){
  if(is.character(res_1000[[x]][[1]])) return(NULL) else res_1000[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 1000)

est_2000 <- lapply(1:length(res_2000), function(x){
  if(is.character(res_2000[[x]][[1]])) return(NULL) else res_2000[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 2000)

est_3000 <- lapply(1:length(res_3000), function(x){
  if(is.character(res_3000[[x]][[1]])) return(NULL) else res_3000[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 3000)

est_4000 <- lapply(1:length(res_4000), function(x){
  if(is.character(res_4000[[x]][[1]])) return(NULL) else res_4000[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 4000)

est_5000 <- lapply(1:length(res_5000), function(x){
  if(is.character(res_5000[[x]][[1]])) return(NULL) else res_5000[[x]][[1]]$out
}) %>% Filter(Negate(is.null), .) %>% 
  do.call('rbind',.) %>% 
  as_tibble() %>% 
  mutate(n = 5000)


# aggregate #
est <- rbind(est_250, est_500, est_1000, est_2000, est_3000, est_4000, est_5000) %>%  
  filter(type == "slope") %>% 
  select(-one_of("type")) %>% 
  mutate_all(as.numeric) %>% 
  mutate(true = -0.215, trueU1 = 5.4334605, type = "slope")

# estimate #
rm_bounds = c(-Inf, Inf)
quant_bound = c(0, 1)
summ <- get_summ(est, rm_bounds = rm_bounds, quant_bound = quant_bound)
ps <- get_plots(summ = summ, results = est, rm_bounds = rm_bounds, quant_bound = quant_bound)
```



### U1 results {.tabset .tabset-fade .tabset-pills}

- **Scenario 1**: As sample size increases both SDR and TMLE converge towards the true value.
- **Scenario 2**: (Weights misspecified) SDR seems to slowly converge towards the true value (maybe eventually will get there). TMLE on the other hand seems to be struggling to converge towards the true value which is pretty suprising. 
- **Scenario 3**: (Outcome misspecified) Starts very off for some reason but does converge towards the true value at a fast rate.
- **Scenario 4**: (Weights misspecified early and outcome misspecified late) Similar start to scenario 3 and converge quickly for SDR but not for TMLE. 
- **Scenario 5**: (Weights misspecified late and outcome misspecified early) Performs a bit less well scenario 1.


**Takeaways**: Results are overall good for SDR, but less so for TMLE. Clearly now the weights are very well estimated as soon as we reach sufficient sample size. The outcome model probably needs to be improved a little to get consistent results in scenario 2.

#### U1 distributions

```{r}
ps$pU1
```

#### Bias of U1

```{r}
ps$pU1bias_slope
```


#### Convergence of U1

```{r}
ps$pU1bias_slope_n
```



### $\hat{\beta}$ results {.tabset .tabset-fade .tabset-pills}


- **Scenario 1**: As sample size increases both SDR and TMLE converge towards the true value. We get consistency (in bias and mean square error) and a good coverage for both estimators. Variance of the estimators are a bit better than IPW. 
- **Scenario 2**: (Weights misspecified) Similar to the U1 results of scenario 2, SDR seems to slowly converge towards the true value (and seems to be stagnating at a small bias). TMLE on the other hand seems to be struggling to converge towards the true value which is pretty surprising. Not consistent SDR but is not aggressively bad. 
- **Scenario 3**: (Outcome misspecified) Consistent for both TMLE and SDR even catching up to the performance of IPW at the price of the variance of the estimates.
- **Scenario 4**: (Weights misspecified early and outcome misspecified late) Consistent for SDR well behaved. TMLE fails in this scenario which is expected.
- **Scenario 5**: (Weights misspecified late and outcome misspecified early) Doesn't perform as well as expected but close to being consistent for SDR and TMLE.


**Takeaways**: Results are overall good for SDR, but less so for TMLE. Perhaps improvinf the outcome model would help in some scenarios.


#### Solution distribution

```{r}
ps$pU2
```

#### Mean Bias

```{r}
ps$pbias_slope
```


#### Mean Bias * sqrt(n)

```{r}
ps$pbias_slope_n
```

#### Median Bias

```{r}
ps$pmedbias_slope
```

#### MSE

```{r}
ps$pMSE_slope
```


#### MSE * n

```{r}
ps$pMSE_slope_n
```


#### Variance of the estimates

```{r}
ps$p_var_slope
```


#### Coverage

```{r}
ps$p_cov_slope
```


#### Figure 1 manuscript

```{r}
library(scales)
p_bias <- ps$pbias_slope + labs(color = "Estimator") + theme(axis.title.x = element_blank()) + 
  theme(axis.title.y = element_text(size = 16))  # Increase Y-axis label size
p_bias_n <- ps$pbias_slope_n + coord_cartesian(ylim = c(0, 6)) + ylab("Scaled bias") + labs(color = "Estimator") + theme(axis.title.x = element_blank()) + 
  theme(axis.title.y = element_text(size = 16))  # Increase Y-axis label size
p_MSE <- ps$pMSE_slope_n + coord_cartesian(ylim = c(0, 30)) + ylab("Scaled MSE") + labs(color = "Estimator") + theme(axis.title.x = element_blank()) + 
  theme(axis.title.y = element_text(size = 16))  # Increase Y-axis label size
p_cov <- ps$p_cov_slope + labs(color = "Estimator") + 
  theme(axis.title.x = element_text(size = 18),  # Increase X-axis label size
        axis.title.y = element_text(size = 16))  # Increase Y-axis label size
fig_1 <- ggarrange(plotlist = list(p_bias, p_bias_n, p_MSE, p_cov), ncol = 1, nrow = 4, common.legend = T, align = "v")
fig_1
```


```{r}
# save the figure to PNG with good resolution #
ggsave(here("results/fig_1.png"), plot = fig_1, width = 12, height = 8, dpi = 500)
```